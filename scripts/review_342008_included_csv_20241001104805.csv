Title,Authors,Abstract,Published Year,Published Month,Journal,Volume,Issue,Pages,Accession Number,DOI,Ref,Covidence #,Study,Notes,Tags
Predicting treatment response from longitudinal images using multi-task deep learning.,Jin C; Yu H; Ke J; Ding P; Yi Y; Jiang X; Duan X; Tang J; Chang DT; Wu X; Gao F; Li R,"Radiographic imaging is routinely used to evaluate treatment response in solid tumors. Current imaging response metrics do not reliably predict the underlying biological response. Here, we present a multi-task deep learning approach that allows simultaneous tumor segmentation and response prediction. We design two Siamese subnetworks that are joined at multiple layers, which enables integration of multi-scale feature representations and in-depth comparison of pre-treatment and post-treatment images. The network is trained using 2568 magnetic resonance imaging scans of 321 rectal cancer patients for predicting pathologic complete response after neoadjuvant chemoradiotherapy. In multi-institution validation, the imaging-based model achieves AUC of 0.95 (95% confidence interval: 0.91-0.98) and 0.92 (0.87-0.96) in two independent cohorts of 160 and 141 patients, respectively. When combined with blood-based tumor markers, the integrated model further improves prediction accuracy with AUC 0.97 (0.93-0.99). Our approach to capturing dynamic information in longitudinal images may be broadly used for screening, treatment response evaluation, disease monitoring, and surveillance.",2021,Mar,Nat Commun,12,1,1851,,10.1038/s41467-021-22188-y,33767170,#1600,Jin 2021,"",""
Longitudinal subcortical segmentation with deep learning.,Li H; Zhang H; Johnson H; Long JD; Paulsen JS; Oguz I,"Longitudinal information is important for monitoring the progression of neurodegenerative diseases, such as Huntington's disease (HD). Specifically, longitudinal magnetic resonance imaging (MRI) studies may allow the discovery of subtle intra-subject changes over time that may otherwise go undetected because of inter-subject variability. For HD patients, the primary imaging-based marker of disease progression is the atrophy of subcortical structures, mainly the caudate and putamen. To better understand the course of subcortical atrophy in HD and its correlation with clinical outcome measures, highly accurate segmentation is important. In recent years, subcortical segmentation methods have moved towards deep learning, given the state-of-the-art accuracy and computational efficiency provided by these models. However, these methods are not designed for longitudinal analysis, but rather treat each time point as an independent sample, discarding the longitudinal structure of the data. In this paper, we propose a deep learning based subcortical segmentation method that takes into account this longitudinal information. Our method takes a longitudinal pair of 3D MRIs as input, and jointly computes the corresponding segmentations. We use bi-directional convolutional long short-term memory (C-LSTM) blocks in our model to leverage the longitudinal information between scans. We test our method on the PREDICT-HD dataset and use the Dice coefficient, average surface distance and 95-percent Hausdorff distance as our evaluation metrics. Compared to cross-sectional segmentation, we improve the overall accuracy of segmentation, and our method has more consistent performance across time points. Furthermore, our method identifies a stronger correlation between subcortical volume loss and decline in the total motor score, an important clinical outcome measure for HD.",2021,Feb,Proc SPIE Int Soc Opt Eng,11596,,,,10.1117/12.2582340,34873358,#1604,Li 2021,"",""
Deep learning for brain metastasis detection and segmentation in longitudinal MRI data.,Huang Y; Bert C; Sommer P; Frey B; Gaipl U; Distel LV; Weissmann T; Uder M; Schmidt MA; Dörfler A; Maier A; Fietkau R; Putz F,"PURPOSE: Brain metastases (BM) occur frequently in patients with metastatic cancer. Early and accurate detection of BM is essential for treatment planning and prognosis in radiation therapy. Due to their tiny sizes and relatively low contrast, small BM are very difficult to detect manually. With the recent development of deep learning technologies, several res earchers have reported promising results in automated brain metastasis detection. However, the detection sensitivity is still not high enough for tiny BM, and integration into clinical practice in regard to differentiating true metastases from false positives (FPs) is challenging. METHODS: The DeepMedic network with the binary cross-entropy (BCE) loss is used as our baseline method. To improve brain metastasis detection performance, a custom detection loss called volume-level sensitivity-specificity (VSS) is proposed, which rates metastasis detection sensitivity and specificity at a (sub)volume level. As sensitivity and precision are always a trade-off, either a high sensitivity or a high precision can be achieved for brain metastasis detection by adjusting the weights in the VSS loss without decline in dice score coefficient for segmented metastases. To reduce metastasis-like structures being detected as FP metastases, a temporal prior volume is proposed as an additional input of DeepMedic. The modified network is called DeepMedic+ for distinction. Combining a high-sensitivity VSS loss and a high specificity loss for DeepMedic+, the majority of true positive metastases are confirmed with high specificity, while additional metastases candidates in each patient are marked with high sensitivity for detailed expert evaluation. RESULTS: Our proposed VSS loss improves the sensitivity of brain metastasis detection, increasing the sensitivity from 85.3% for DeepMedic with BCE to 97.5% for DeepMedic with VSS. Alternatively, the precision is improved from 69.1% for DeepMedic with BCE to 98.7% for DeepMedic with VSS. Comparing DeepMedic+ with DeepMedic with the same VSS loss, 44.4% of the FP metastases are reduced in the high-sensitivity model and the precision reaches 99.6% for the high-specificity model. The mean dice coefficient for all metastases is about 0.81. With the ensemble of the high-sensitivity and high-specificity models, on average only 1.5 FP metastases per patient need further check, while the majority of true positive metastases are confirmed. CONCLUSIONS: Our proposed VSS loss and temporal prior improve brain metastasis detection sensitivity and precision. The ensemble learning is able to distinguish high confidence true positive metastases from metastases candidates that require special expert review or further follow-up, being particularly well-fit to the requirements of expert support in real clinical practice. This facilitates metastasis detection and segmentation for neuroradiologists in diagnostic and radiation oncologists in therapeutic clinical applications.",2022,Sep,Med Phys,49,9,5773-5786,,10.1002/mp.15863,35833351,#1606,Huang 2022,"",""
Automated segmentation of biventricular contours in tissue phase mapping using deep learning.,Shen D; Pathrose A; Sarnari R; Blake A; Berhane H; Baraboo JJ; Carr JC; Markl M; Kim D,"Tissue phase mapping (TPM) is an MRI technique for quantification of regional biventricular myocardial velocities. Despite its potential, clinical use is limited due to the requisite labor-intensive manual segmentation of cardiac contours for all time frames. The purpose of this study was to develop a deep learning (DL) network for automated segmentation of TPM images, without significant loss in segmentation and myocardial velocity quantification accuracy compared with manual segmentation. We implemented a multi-channel 3D (three dimensional; 2D + time) dense U-Net that trained on magnitude and phase images and combined cross-entropy, Dice, and Hausdorff distance loss terms to improve the segmentation accuracy and suppress unnatural boundaries. The dense U-Net was trained and tested with 150 multi-slice, multi-phase TPM scans (114 scans for training, 36 for testing) from 99 heart transplant patients (44 females, 1-4 scans/patient), where the magnitude and velocity-encoded (V(x) , V(y) , V(z) ) images were used as input and the corresponding manual segmentation masks were used as reference. The accuracy of DL segmentation was evaluated using quantitative metrics (Dice scores, Hausdorff distance) and linear regression and Bland-Altman analyses on the resulting peak radial and longitudinal velocities (V(r) and V(z) ). The mean segmentation time was about 2 h per patient for manual and 1.9 ± 0.3 s for DL. Our network produced good accuracy (median Dice = 0.85 for left ventricle (LV), 0.64 for right ventricle (RV), Hausdorff distance = 3.17 pixels) compared with manual segmentation. Peak V(r) and V(z) measured from manual and DL segmentations were strongly correlated (R ≥ 0.88) and in good agreement with manual analysis (mean difference and limits of agreement for V(z) and V(r) were -0.05 ± 0.98 cm/s and -0.06 ± 1.18 cm/s for LV, and -0.21 ± 2.33 cm/s and 0.46 ± 4.00 cm/s for RV, respectively). The proposed multi-channel 3D dense U-Net was capable of reducing the segmentation time by 3,600-fold, without significant loss in accuracy in tissue velocity measurements.",2021,Dec,NMR Biomed,34,12,e4606,,10.1002/nbm.4606,34476863,#1614,Shen 2021,"",""
Patch-based 3D U-Net and transfer learning for longitudinal piglet brain segmentation on MRI.,Coupeau P; Fasquel JB; Mazerand E; Menei P; Montero-Menei CN; Dinomais M,"BACKGROUND AND OBJECTIVES: In order to study neural plasticity in immature brain following early brain lesion, large animal model are needed. Because of its morphological similarities with the human developmental brain, piglet is a suitable but little used one. Its study from Magnetic Resonance Imaging (MRI) requires the development of automatic algorithms for the segmentation of the different structures and tissues. A crucial preliminary step consists in automatically segmenting the brain. METHODS: We propose a fully automatic brain segmentation method applied to piglets by combining a 3D patch-based U-Net and a post-processing pipeline for spatial regularization and elimination of false positives. Our approach also integrates a transfer-learning strategy for managing an automated longitudinal monitoring evaluated for four developmental stages (2, 6, 10 and 18 weeks), facing the issue of MRI changes resulting from the rapid brain development. It is compared to a 2D approach and the Brain Extraction Tool (BET) as well as techniques adapted to other animals (rodents, macaques). The influence of training patches size and distribution is studied as well as the benefits of spatial regularization. RESULTS: Results show that our approach is efficient in terms of average Dice score (0.952) and Hausdorff distance (8.51), outperforming the use of a 2D U-Net (Dice: 0.919, Hausdorff distance: 11.06) and BET (Dice: 0.764, Hausdorff distance: 25.91). The transfer-learning strategy achieves a good performance on older piglets (Dice of 0.934 at 6 weeks, 0.956 at 10 weeks and 0.958 at 18 weeks) compared to a standard training strategy with few data (Dice of 0.636 at 6 weeks, 0.907 at 10 weeks, not calculable at 18 weeks because of too few training piglets). CONCLUSIONS: In conclusion, we provide a method for longitudinal MRI piglet brain segmentation based on 3D U-Net and transfer learning which can be used for future morphometric studies and applied to other animals.",2022,Feb,Comput Methods Programs Biomed,214,,106563,,10.1016/j.cmpb.2021.106563,34890993,#1620,Coupeau 2022,"",""
Evaluation of Spatial Attentive Deep Learning for Automatic Placental Segmentation on Longitudinal MRI.,Liu Y; Zabihollahy F; Yan R; Lee B; Janzen C; Devaskar SU; Sung K,"BACKGROUND: Automated segmentation of the placenta by MRI in early pregnancy may help predict normal and aberrant placenta function, which could improve the efficiency of placental assessment and the prediction of pregnancy outcomes. An automated segmentation method that works at one gestational age may not transfer effectively to other gestational ages. PURPOSE: To evaluate a spatial attentive deep learning method (SADL) for automated placental segmentation on longitudinal placental MRI scans. STUDY TYPE: Prospective, single-center. SUBJECTS: A total of 154 pregnant women who underwent MRI scans at both 14-18 weeks of gestation and at 19-24 weeks of gestation, divided into training (N = 108), validation (N = 15), and independent testing datasets (N = 31). FIELD STRENGTH/SEQUENCE: A 3 T, T2-weighted half Fourier single-shot turbo spin-echo (T2-HASTE) sequence. ASSESSMENT: The reference standard of placental segmentation was manual delineation on T2-HASTE by a third-year neonatology clinical fellow (B.L.) under the supervision of an experienced maternal-fetal medicine specialist (C.J. with 20 years of experience) and an MRI scientist (K.S. with 19 years of experience). STATISTICAL TESTS: The three-dimensional Dice similarity coefficient (DSC) was used to measure the automated segmentation performance compared to the manual placental segmentation. A paired t-test was used to compare the DSCs between SADL and U-Net methods. A Bland-Altman plot was used to analyze the agreement between manual and automated placental volume measurements. A P value < 0.05 was considered statistically significant. RESULTS: In the testing dataset, SADL achieved average DSCs of 0.83 ± 0.06 and 0.84 ± 0.05 in the first and second MRI, which were significantly higher than those achieved by U-Net (0.77 ± 0.08 and 0.76 ± 0.10, respectively). A total of 6 out of 62 MRI scans (9.6%) had volume measurement differences between the SADL-based automated and manual volume measurements that were out of 95% limits of agreement. DATA CONCLUSIONS: SADL can automatically detect and segment the placenta with high performance in MRI at two different gestational ages. LEVEL OF EVIDENCE: 4 TECHNICAL EFFICACY STAGE: 2.",2023,May,J Magn Reson Imaging,57,5,1533-1540,,10.1002/jmri.28403,37021577,#1629,Liu 2023,"",""
PTNet3D: A 3D High-Resolution Longitudinal Infant Brain MRI Synthesizer Based on Transformers.,Zhang X; He X; Guo J; Ettehadi N; Aw N; Semanek D; Posner J; Laine A; Wang Y,"An increased interest in longitudinal neurodevelopment during the first few years after birth has emerged in recent years. Noninvasive magnetic resonance imaging (MRI) can provide crucial information about the development of brain structures in the early months of life. Despite the success of MRI collections and analysis for adults, it remains a challenge for researchers to collect high-quality multimodal MRIs from developing infant brains because of their irregular sleep pattern, limited attention, inability to follow instructions to stay still during scanning. In addition, there are limited analytic approaches available. These challenges often lead to a significant reduction of usable MRI scans and pose a problem for modeling neurodevelopmental trajectories. Researchers have explored solving this problem by synthesizing realistic MRIs to replace corrupted ones. Among synthesis methods, the convolutional neural network-based (CNN-based) generative adversarial networks (GANs) have demonstrated promising performance. In this study, we introduced a novel 3D MRI synthesis framework- pyramid transformer network (PTNet3D)- which relies on attention mechanisms through transformer and performer layers. We conducted extensive experiments on high-resolution Developing Human Connectome Project (dHCP) and longitudinal Baby Connectome Project (BCP) datasets. Compared with CNN-based GANs, PTNet3D consistently shows superior synthesis accuracy and superior generalization on two independent, large-scale infant brain MRI datasets. Notably, we demonstrate that PTNet3D synthesized more realistic scans than CNN-based models when the input is from multi-age subjects. Potential applications of PTNet3D include synthesizing corrupted or missing images. By replacing corrupted scans with synthesized ones, we observed significant improvement in infant whole brain segmentation.",2022,Oct,IEEE Trans Med Imaging,41,10,2925-2940,,10.1109/TMI.2022.3174827,35560070,#1635,Zhang 2022,"",""
Fully automated longitudinal segmentation of new or enlarged multiple sclerosis lesions using 3D convolutional neural networks.,Krüger J; Opfer R; Gessert N; Ostwaldt AC; Manogaran P; Kitzler HH; Schlaefer A; Schippling S,"The quantification of new or enlarged lesions from follow-up MRI scans is an important surrogate of clinical disease activity in patients with multiple sclerosis (MS). Not only is manual segmentation time consuming, but inter-rater variability is high. Currently, only a few fully automated methods are available. We address this gap in the field by employing a 3D convolutional neural network (CNN) with encoder-decoder architecture for fully automatic longitudinal lesion segmentation. Input data consist of two fluid attenuated inversion recovery (FLAIR) images (baseline and follow-up) per patient. Each image is entered into the encoder and the feature maps are concatenated and then fed into the decoder. The output is a 3D mask indicating new or enlarged lesions (compared to the baseline scan). The proposed method was trained on 1809 single point and 1444 longitudinal patient data sets and then validated on 185 independent longitudinal data sets from two different scanners. From the two validation data sets, manual segmentations were available from three experienced raters, respectively. The performance of the proposed method was compared to the open source Lesion Segmentation Toolbox (LST), which is a current state-of-art longitudinal lesion segmentation method. The mean lesion-wise inter-rater sensitivity was 62%, while the mean inter-rater number of false positive (FP) findings was 0.41 lesions per case. The two validated algorithms showed a mean sensitivity of 60% (CNN), 46% (LST) and a mean FP of 0.48 (CNN), 1.86 (LST) per case. Sensitivity and number of FP were not significantly different (p < 0.05) between the CNN and manual raters. New or enlarged lesions counted by the CNN algorithm appeared to be comparable with manual expert ratings. The proposed algorithm seems to outperform currently available approaches, particularly LST. The high inter-rater variability in case of manual segmentation indicates the complexity of identifying new or enlarged lesions. An automated CNN-based approach can quickly provide an independent and deterministic assessment of new or enlarged lesions from baseline to follow-up scans with acceptable reliability.",2020,,Neuroimage Clin,28,,102445,,10.1016/j.nicl.2020.102445,33038667,#1641,Krüger 2020,"",""
Longitudinal diffusion MRI analysis using Segis-Net: A single-step deep-learning framework for simultaneous segmentation and registration.,Li B; Niessen WJ; Klein S; de Groot M; Ikram MA; Vernooij MW; Bron EE,"This work presents a single-step deep-learning framework for longitudinal image analysis, coined Segis-Net. To optimally exploit information available in longitudinal data, this method concurrently learns a multi-class segmentation and nonlinear registration. Segmentation and registration are modeled using a convolutional neural network and optimized simultaneously for their mutual benefit. An objective function that optimizes spatial correspondence for the segmented structures across time-points is proposed. We applied Segis-Net to the analysis of white matter tracts from N=8045 longitudinal brain MRI datasets of 3249 elderly individuals. Segis-Net approach showed a significant increase in registration accuracy, spatio-temporal segmentation consistency, and reproducibility compared with two multistage pipelines. This also led to a significant reduction in the sample-size that would be required to achieve the same statistical power in analyzing tract-specific measures. Thus, we expect that Segis-Net can serve as a new reliable tool to support longitudinal imaging studies to investigate macro- and microstructural brain changes over time.",2021,Jul,Neuroimage,235,,118004,,10.1016/j.neuroimage.2021.118004,33794359,#1648,Li 2021,"",""
Progressive multifocal leukoencephalopathy lesion and brain parenchymal segmentation from MRI using serial deep convolutional neural networks.,Al-Louzi O; Roy S; Osuorah I; Parvathaneni P; Smith BR; Ohayon J; Sati P; Pham DL; Jacobson S; Nath A; Reich DS; Cortese I,"Progressive multifocal leukoencephalopathy (PML) is a rare opportunistic brain infection caused by the JC virus and associated with substantial morbidity and mortality. Accurate MRI assessment of PML lesion burden and brain parenchymal atrophy is of decisive value in monitoring the disease course and response to therapy. However, there are currently no validated automatic methods for quantification of PML lesion burden or associated parenchymal volume loss. Furthermore, manual brain or lesion delineations can be tedious, require the use of valuable time resources by radiologists or trained experts, and are often subjective. In this work, we introduce JCnet (named after the causative viral agent), an end-to-end, fully automated method for brain parenchymal and lesion segmentation in PML using consecutive 3D patch-based convolutional neural networks. The network architecture consists of multi-view feature pyramid networks with hierarchical residual learning blocks containing embedded batch normalization and nonlinear activation functions. The feature maps across the bottom-up and top-down pathways of the feature pyramids are merged, and an output probability membership generated through convolutional pathways, thus rendering the method fully convolutional. Our results show that this approach outperforms and improves longitudinal consistency compared to conventional, state-of-the-art methods of healthy brain and multiple sclerosis lesion segmentation, utilized here as comparators given the lack of available methods validated for use in PML. The ability to produce robust and accurate automated measures of brain atrophy and lesion segmentation in PML is not only valuable clinically but holds promise toward including standardized quantitative MRI measures in clinical trials of targeted therapies. Code is available at: https://github.com/omarallouz/JCnet.",2020,,Neuroimage Clin,28,,102499,,10.1016/j.nicl.2020.102499,33395989,#1680,Al-Louzi 2020,"",""
Clinical Evaluation of a Fully-automatic Segmentation Method for Longitudinal Brain Tumor Volumetry.,Meier R; Knecht U; Loosli T; Bauer S; Slotboom J; Wiest R; Reyes M,"Information about the size of a tumor and its temporal evolution is needed for diagnosis as well as treatment of brain tumor patients. The aim of the study was to investigate the potential of a fully-automatic segmentation method, called BraTumIA, for longitudinal brain tumor volumetry by comparing the automatically estimated volumes with ground truth data acquired via manual segmentation. Longitudinal Magnetic Resonance (MR) Imaging data of 14 patients with newly diagnosed glioblastoma encompassing 64 MR acquisitions, ranging from preoperative up to 12 month follow-up images, was analysed. Manual segmentation was performed by two human raters. Strong correlations (R = 0.83-0.96, p < 0.001) were observed between volumetric estimates of BraTumIA and of each of the human raters for the contrast-enhancing (CET) and non-enhancing T2-hyperintense tumor compartments (NCE-T2). A quantitative analysis of the inter-rater disagreement showed that the disagreement between BraTumIA and each of the human raters was comparable to the disagreement between the human raters. In summary, BraTumIA generated volumetric trend curves of contrast-enhancing and non-enhancing T2-hyperintense tumor compartments comparable to estimates of human raters. These findings suggest the potential of automated longitudinal tumor segmentation to substitute manual volumetric follow-up of contrast-enhancing and non-enhancing T2-hyperintense tumor compartments.",2016,Mar,Sci Rep,6,,23376,,10.1038/srep23376,27001047,#1682,Meier 2016,"",""
"Automatic segmentation, classification, and follow-up of optic pathway gliomas using deep learning and fuzzy c-means clustering based on MRI.",Artzi M; Gershov S; Ben-Sira L; Roth J; Kozyrev D; Shofty B; Gazit T; Halag-Milo T; Constantini S; Ben Bashat D,"PURPOSE: Optic pathway gliomas (OPG) are low-grade pilocytic astrocytomas accounting for 3-5% of pediatric intracranial tumors. Accurate and quantitative follow-up of OPG using magnetic resonance imaging (MRI) is crucial for therapeutic decision making, yet is challenging due to the complex shape and heterogeneous tissue pattern which characterizes these tumors. The aim of this study was to implement automatic methods for segmentation and classification of OPG and its components, based on MRI. METHODS: A total of 202 MRI scans from 29 patients with chiasmatic OPG scanned longitudinally were retrospectively collected and included in this study. Data included T(2) and post-contrast T(1) weighted images. The entire tumor volume and its components were manually annotated by a senior neuro-radiologist, and inter- and intra-rater variability of the entire tumor volume was assessed in a subset of scans. Automatic tumor segmentation was performed using deep-learning method with U-Net+ResNet architecture. A fivefold cross-validation scheme was used to evaluate the automatic results relative to manual segmentation. Voxel-based classification of the tumor into enhanced, non-enhanced, and cystic components was performed using fuzzy c-means clustering. RESULTS: The results of the automatic tumor segmentation were: mean dice score = 0.736 ± 0.025, precision = 0.918 ± 0.014, and recall = 0.635 ± 0.039 for the validation data, and dice score = 0.761 ± 0.011, precision = 0.794 ± 0.028, and recall = 0.742 ± 0.012 for the test data. The accuracy of the voxel-based classification of tumor components was 0.94, with precision = 0.89, 0.97, and 0.85, and recall = 1.00, 0.79, and 0.94 for the non-enhanced, enhanced, and cystic components, respectively. CONCLUSION: This study presents methods for automatic segmentation of chiasmatic OPG tumors and classification into the different components of the tumor, based on conventional MRI. Automatic quantitative longitudinal assessment of these tumors may improve radiological monitoring, facilitate early detection of disease progression and optimize therapy management.",2020,Nov,Med Phys,47,11,5693-5701,,10.1002/mp.14489,32969025,#1697,Artzi 2020,"",""
"Deep learning prediction of pathological complete response, residual cancer burden, and progression-free survival in breast cancer patients.",Dammu H; Ren T; Duong TQ,"The goal of this study was to employ novel deep-learning convolutional-neural-network (CNN) to predict pathological complete response (PCR), residual cancer burden (RCB), and progression-free survival (PFS) in breast cancer patients treated with neoadjuvant chemotherapy using longitudinal multiparametric MRI, demographics, and molecular subtypes as inputs. In the I-SPY-1 TRIAL, 155 patients with stage 2 or 3 breast cancer with breast tumors underwent neoadjuvant chemotherapy met the inclusion/exclusion criteria. The inputs were dynamic-contrast-enhanced (DCE) MRI, and T2- weighted MRI as three-dimensional whole-images without the tumor segmentation, as well as molecular subtypes and demographics. The outcomes were PCR, RCB, and PFS. Three (""Integrated"", ""Stack"" and ""Concatenation"") CNN were evaluated using receiver-operating characteristics and mean absolute errors. The Integrated approach outperformed the ""Stack"" or ""Concatenation"" CNN. Inclusion of both MRI and non-MRI data outperformed either alone. The combined pre- and post-neoadjuvant chemotherapy data outperformed either alone. Using the best model and data combination, PCR prediction yielded an accuracy of 0.81±0.03 and AUC of 0.83±0.03; RCB prediction yielded an accuracy of 0.80±0.02 and Cohen's κ of 0.73±0.03; PFS prediction yielded a mean absolute error of 24.6±0.7 months (survival ranged from 6.6 to 127.5 months). Deep learning using longitudinal multiparametric MRI, demographics, and molecular subtypes accurately predicts PCR, RCB, and PFS in breast cancer patients. This approach may prove useful for treatment selection, planning, execution, and mid-treatment adjustment.",2023,,PLoS One,18,1,e0280148,,10.1371/journal.pone.0280148,36607982,#1718,Dammu 2023,"",""
Surface spherical encoding and contrastive learning for virtual bone shape aging.,Calivá F; Kamat S; Morales Martinez A; Majumdar S; Pedoia V,"Bone shape changes are considered a relevant biomarker in understanding the onset and progression of knee osteoarthritis (OA). This study used a novel deep learning pipeline to predict longitudinal bone shape changes in the femur four years in advance, using bone surfaces that were extracted in knee MRIs from the OA initiative study, via a segmentation procedure and encoded as shape maps using spherical coordinates. Given a sequence of three consecutive shape maps (collected in a time window of 24 months), a fully convolutional network was trained to predict the whole bone surface 48 months after the last observed time point, and a classifier to diagnose OA in the predicted maps. For this, a novel multi-term loss function, based on contrastive learning was designed. Experimental results show that the model predicted shape changes with an L(1) error comparable to the MRI slice thickness (0.7mm). Next, an ablation study demonstrated that the introduction of a contrastive term in the loss improved sensitivity of the OA classifier, increasing sensitivity from 0.537 to 0.709, just shy of the upper bound of 0.740 computed on the ground truth bone shape maps. Our approach provides a promising tool, suitable for patient specific OA trajectory analysis.",2022,Apr,Med Image Anal,77,,102388,,10.1016/j.media.2022.102388,35172227,#1731,Calivá 2022,"",""
Deep learning prediction of mild cognitive impairment conversion to Alzheimer's disease at 3 years after diagnosis using longitudinal and whole-brain 3D MRI.,Ocasio E; Duong TQ,"BACKGROUND: While there is no cure for Alzheimer's disease (AD), early diagnosis and accurate prognosis of AD may enable or encourage lifestyle changes, neurocognitive enrichment, and interventions to slow the rate of cognitive decline. The goal of our study was to develop and evaluate a novel deep learning algorithm to predict mild cognitive impairment (MCI) to AD conversion at three years after diagnosis using longitudinal and whole-brain 3D MRI. METHODS: This retrospective study consisted of 320 normal cognition (NC), 554 MCI, and 237 AD patients. Longitudinal data include T1-weighted 3D MRI obtained at initial presentation with diagnosis of MCI and at 12-month follow up. Whole-brain 3D MRI volumes were used without a priori segmentation of regional structural volumes or cortical thicknesses. MRIs of the AD and NC cohort were used to train a deep learning classification model to obtain weights to be applied via transfer learning for prediction of MCI patient conversion to AD at three years post-diagnosis. Two (zero-shot and fine tuning) transfer learning methods were evaluated. Three different convolutional neural network (CNN) architectures (sequential, residual bottleneck, and wide residual) were compared. Data were split into 75% and 25% for training and testing, respectively, with 4-fold cross validation. Prediction accuracy was evaluated using balanced accuracy. Heatmaps were generated. RESULTS: The sequential convolutional approach yielded slightly better performance than the residual-based architecture, the zero-shot transfer learning approach yielded better performance than fine tuning, and CNN using longitudinal data performed better than CNN using a single timepoint MRI in predicting MCI conversion to AD. The best CNN model for predicting MCI conversion to AD at three years after diagnosis yielded a balanced accuracy of 0.793. Heatmaps of the prediction model showed regions most relevant to the network including the lateral ventricles, periventricular white matter and cortical gray matter. CONCLUSIONS: This is the first convolutional neural network model using longitudinal and whole-brain 3D MRIs without extracting regional brain volumes or cortical thicknesses to predict future MCI to AD conversion at 3 years after diagnosis. This approach could lead to early prediction of patients who are likely to progress to AD and thus may lead to better management of the disease.",2021,,PeerJ Comput Sci,7,,e560,,10.7717/peerj-cs.560,34141888,#1747,Ocasio 2021,"",""
An end-end deep learning framework for lesion segmentation on multi-contrast MR images-an exploratory study in a rat model of traumatic brain injury.,Kn BP; Cs A; Mohammed A; Chitta KK; To XV; Srour H; Nasrallah F,"Traumatic brain injury (TBI) engenders traumatic necrosis and penumbra-areas of secondary neural injury which are crucial targets for therapeutic interventions. Segmenting manually areas of ongoing changes like necrosis, edema, hematoma, and inflammation is tedious, error-prone, and biased. Using the multi-parametric MR data from a rodent model study, we demonstrate the effectiveness of an end-end deep learning global-attention-based UNet (GA-UNet) framework for automatic segmentation and quantification of TBI lesions. Longitudinal MR scans (2 h, 1, 3, 7, 14, 30, and 60 days) were performed on eight Sprague-Dawley rats after controlled cortical injury was performed. TBI lesion and sub-regions segmentation was performed using 3D-UNet and GA-UNet. Dice statistics (DSI) and Hausdorff distance were calculated to assess the performance. MR scan variations-based (bias, noise, blur, ghosting) data augmentation was performed to develop a robust model.Training/validation median DSI for U-Net was 0.9368 with T2w and MPRAGE inputs, whereas GA-UNet had 0.9537 for the same. Testing accuracies were higher for GA-UNet than U-Net with a DSI of 0.8232 for the T2w-MPRAGE inputs.Longitudinally, necrosis remained constant while oligemia and penumbra decreased, and edema appearing around day 3 which increased with time. GA-UNet shows promise for multi-contrast MR image-based segmentation/quantification of TBI in large cohort studies.",2023,Mar,Med Biol Eng Comput,61,3,847-865,,10.1007/s11517-022-02752-4,36624356,#1754,Kn 2023,"",""
Generating Longitudinal Atrophy Evaluation Datasets on Brain Magnetic Resonance Images Using Convolutional Neural Networks and Segmentation Priors.,Bernal J; Valverde S; Kushibar K; Cabezas M; Oliver A; Lladó X,"Brain atrophy quantification plays a fundamental role in neuroinformatics since it permits studying brain development and neurological disorders. However, the lack of a ground truth prevents testing the accuracy of longitudinal atrophy quantification methods. We propose a deep learning framework to generate longitudinal datasets by deforming T1-w brain magnetic resonance imaging scans as requested through segmentation maps. Our proposal incorporates a cascaded multi-path U-Net optimised with a multi-objective loss which allows its paths to generate different brain regions accurately. We provided our model with baseline scans and real follow-up segmentation maps from two longitudinal datasets, ADNI and OASIS, and observed that our framework could produce synthetic follow-up scans that matched the real ones (Total scans= 584; Median absolute error: 0.03 ± 0.02; Structural similarity index: 0.98 ± 0.02; Dice similarity coefficient: 0.95 ± 0.02; Percentage of brain volume change: 0.24 ± 0.16; Jacobian integration: 1.13 ± 0.05). Compared to two relevant works generating brain lesions using U-Nets and conditional generative adversarial networks (CGAN), our proposal outperformed them significantly in most cases (p < 0.01), except in the delineation of brain edges where the CGAN took the lead (Jacobian integration: Ours - 1.13 ± 0.05 vs CGAN - 1.00 ± 0.02; p < 0.01). We examined whether changes induced with our framework were detected by FAST, SPM, SIENA, SIENAX, and the Jacobian integration method. We observed that induced and detected changes were highly correlated (Adj. R(2) > 0.86). Our preliminary results on harmonised datasets showed the potential of our framework to be applied to various data collections without further adjustment.",2021,Jul,Neuroinformatics,19,3,477-492,,10.1007/s12021-020-09499-z,33389607,#1756,Bernal 2021,"",""
Segmenting lung tumors on longitudinal imaging studies via a patient-specific adaptive convolutional neural network.,Wang C; Tyagi N; Rimner A; Hu YC; Veeraraghavan H; Li G; Hunt M; Mageras G; Zhang P,"PURPOSE: To design a deep learning algorithm that automatically delineates lung tumors seen on weekly magnetic resonance imaging (MRI) scans acquired during radiotherapy and facilitates the analysis of geometric tumor changes. METHODS: This longitudinal imaging study comprised 9 lung cancer patients who had 6-7 weekly T2-weighted MRI scans during radiotherapy. Tumors on all scans were manually contoured as the ground truth. Meanwhile, a patient-specific adaptive convolutional neural network (A-net) was developed to simulate the workflow of adaptive radiotherapy and to utilize past weekly MRI and tumor contours to segment tumors on the current weekly MRI. To augment the training data, each voxel inside the volume of interest was expanded to a 3 × 3 cm patch as the input, whereas the classification of the corresponding patch, background or tumor, was the output. Training was updated weekly to incorporate the latest MRI scan. For comparison, a population-based neural network was implemented, trained, and validated on the leave-one-out scheme. Both algorithms were evaluated by their precision, DICE coefficient, and root mean square surface distance between the manual and computerized segmentations. RESULTS: Training of A-net converged well within 2 h of computations on a computer cluster. A-net segmented the weekly MR with a precision, DICE, and root mean square surface distance of 0.81 ± 0.10, 0.82 ± 0.10, and 2.4 ± 1.4 mm, and outperformed the population-based algorithm with 0.63 ± 0.21, 0.64 ± 0.19, and 4.1 ± 3.0 mm, respectively. CONCLUSION: A-net can be feasibly integrated into the clinical workflow of a longitudinal imaging study and become a valuable tool to facilitate decision- making in adaptive radiotherapy.",2019,Feb,Radiother Oncol,131,,101-107,,10.1016/j.radonc.2018.10.037,30773175,#1776,Wang 2019,"",""
A fully convolutional neural network for new T2-w lesion detection in multiple sclerosis.,Salem M; Valverde S; Cabezas M; Pareto D; Oliver A; Salvi J; Rovira À; Lladó X,"INTRODUCTION: Longitudinal magnetic resonance imaging (MRI) has an important role in multiple sclerosis (MS) diagnosis and follow-up. Specifically, the presence of new T2-w lesions on brain MR scans is considered a predictive biomarker for the disease. In this study, we propose a fully convolutional neural network (FCNN) to detect new T2-w lesions in longitudinal brain MR images. METHODS: One year apart, multichannel brain MR scans (T1-w, T2-w, PD-w, and FLAIR) were obtained for 60 patients, 36 of them with new T2-w lesions. Modalities from both temporal points were preprocessed and linearly coregistered. Afterwards, an FCNN, whose inputs were from the baseline and follow-up images, was trained to detect new MS lesions. The first part of the network consisted of U-Net blocks that learned the deformation fields (DFs) and nonlinearly registered the baseline image to the follow-up image for each input modality. The learned DFs together with the baseline and follow-up images were then fed to the second part, another U-Net that performed the final detection and segmentation of new T2-w lesions. The model was trained end-to-end, simultaneously learning both the DFs and the new T2-w lesions, using a combined loss function. We evaluated the performance of the model following a leave-one-out cross-validation scheme. RESULTS: In terms of the detection of new lesions, we obtained a mean Dice similarity coefficient of 0.83 with a true positive rate of 83.09% and a false positive detection rate of 9.36%. In terms of segmentation, we obtained a mean Dice similarity coefficient of 0.55. The performance of our model was significantly better compared to the state-of-the-art methods (p < 0.05). CONCLUSIONS: Our proposal shows the benefits of combining a learning-based registration network with a segmentation network. Compared to other methods, the proposed model decreases the number of false positives. During testing, the proposed model operates faster than the other two state-of-the-art methods based on the DF obtained by Demons.",2020,,Neuroimage Clin,25,,102149,,10.1016/j.nicl.2019.102149,31918065,#1787,Salem 2020,"",""
Automatic detection of lesion load change in Multiple Sclerosis using convolutional neural networks with segmentation confidence.,McKinley R; Wepfer R; Grunder L; Aschwanden F; Fischer T; Friedli C; Muri R; Rummel C; Verma R; Weisstanner C; Wiestler B; Berger C; Eichinger P; Muhlau M; Reyes M; Salmen A; Chan A; Wiest R; Wagner F,"The detection of new or enlarged white-matter lesions is a vital task in the monitoring of patients undergoing disease-modifying treatment for multiple sclerosis. However, the definition of 'new or enlarged' is not fixed, and it is known that lesion-counting is highly subjective, with high degree of inter- and intra-rater variability. Automated methods for lesion quantification, if accurate enough, hold the potential to make the detection of new and enlarged lesions consistent and repeatable. However, the majority of lesion segmentation algorithms are not evaluated for their ability to separate radiologically progressive from radiologically stable patients, despite this being a pressing clinical use-case. In this paper, we explore the ability of a deep learning segmentation classifier to separate stable from progressive patients by lesion volume and lesion count, and find that neither measure provides a good separation. Instead, we propose a method for identifying lesion changes of high certainty, and establish on an internal dataset of longitudinal multiple sclerosis cases that this method is able to separate progressive from stable time-points with a very high level of discrimination (AUC = 0.999), while changes in lesion volume are much less able to perform this separation (AUC = 0.71). Validation of the method on two external datasets confirms that the method is able to generalize beyond the setting in which it was trained, achieving an accuracies of 75 % and 85 % in separating stable and progressive time-points.",2020,,Neuroimage Clin,25,,102104,,10.1016/j.nicl.2019.102104,31927500,#1830,McKinley 2020,"",""
Automated segmentation of the parotid gland based on atlas registration and machine learning: a longitudinal MRI study in head-and-neck radiation therapy.,Yang X; Wu N; Cheng G; Zhou Z; Yu DS; Beitler JJ; Curran WJ; Liu T,"PURPOSE: To develop an automated magnetic resonance imaging (MRI) parotid segmentation method to monitor radiation-induced parotid gland changes in patients after head and neck radiation therapy (RT). METHODS AND MATERIALS: The proposed method combines the atlas registration method, which captures the global variation of anatomy, with a machine learning technology, which captures the local statistical features, to automatically segment the parotid glands from the MRIs. The segmentation method consists of 3 major steps. First, an atlas (pre-RT MRI and manually contoured parotid gland mask) is built for each patient. A hybrid deformable image registration is used to map the pre-RT MRI to the post-RT MRI, and the transformation is applied to the pre-RT parotid volume. Second, the kernel support vector machine (SVM) is trained with the subject-specific atlas pair consisting of multiple features (intensity, gradient, and others) from the aligned pre-RT MRI and the transformed parotid volume. Third, the well-trained kernel SVM is used to differentiate the parotid from surrounding tissues in the post-RT MRIs by statistically matching multiple texture features. A longitudinal study of 15 patients undergoing head and neck RT was conducted: baseline MRI was acquired prior to RT, and the post-RT MRIs were acquired at 3-, 6-, and 12-month follow-up examinations. The resulting segmentations were compared with the physicians' manual contours. RESULTS: Successful parotid segmentation was achieved for all 15 patients (42 post-RT MRIs). The average percentage of volume differences between the automated segmentations and those of the physicians' manual contours were 7.98% for the left parotid and 8.12% for the right parotid. The average volume overlap was 91.1% ± 1.6% for the left parotid and 90.5% ± 2.4% for the right parotid. The parotid gland volume reduction at follow-up was 25% at 3 months, 27% at 6 months, and 16% at 12 months. CONCLUSIONS: We have validated our automated parotid segmentation algorithm in a longitudinal study. This segmentation method may be useful in future studies to address radiation-induced xerostomia in head and neck radiation therapy.",2014,Dec,Int J Radiat Oncol Biol Phys,90,5,1225-33,,10.1016/j.ijrobp.2014.08.350,25442347,#1858,Yang 2014,"Thomas Shaw (2024-07-12 00:07:37)(Select): This uses registration first then SVM - it technically fits, but i think we need a note about why it doesn't actually do what we think it does; ",""
GP-GAN: Brain tumor growth prediction using stacked 3D generative adversarial networks from longitudinal MR Images.,Elazab A; Wang C; Gardezi SJS; Bai H; Hu Q; Wang T; Chang C; Lei B,"Brain tumors are one of the major common causes of cancer-related death, worldwide. Growth prediction of these tumors, particularly gliomas which are the most dominant type, can be quite useful to improve treatment planning, quantify tumor aggressiveness, and estimate patients' survival time towards precision medicine. Studying tumor growth prediction basically requires multiple time points of single or multimodal medical images of the same patient. Recent models are based on complex mathematical formulations that basically rely on a system of partial differential equations, e.g. reaction diffusion model, to capture the diffusion and proliferation of tumor cells in the surrounding tissue. However, these models usually have small number of parameters that are insufficient to capture different patterns and other characteristics of the tumors. In addition, such models consider tumor growth independently for each subject, not being able to get benefit from possible common growth patterns existed in the whole population under study. In this paper, we propose a novel data-driven method via stacked 3D generative adversarial networks (GANs), named GP-GAN, for growth prediction of glioma. Specifically, we use stacked conditional GANs with a novel objective function that includes both l(1) and Dice losses. Moreover, we use segmented feature maps to guide the generator for better generated images. Our generator is designed based on a modified 3D U-Net architecture with skip connections to combine hierarchical features and thus have a better generated image. The proposed method is trained and tested on 18 subjects with 3 time points (9 subjects from collaborative hospital and 9 subjects from BRATS 2014 dataset). Results show that our proposed GP-GAN outperforms state-of-the-art methods for glioma growth prediction and attain average Jaccard index and Dice coefficient of 78.97% and 88.26%, respectively.",2020,Dec,Neural Netw,132,,321-332,,10.1016/j.neunet.2020.09.004,32977277,#2078,Elazab 2020,"",""
Towards Longitudinal Glioma Segmentation: Evaluating combined pre- and post-treatment MRI training data for automated tumor segmentation using nnU-Net.,Ranjbar S; Singleton KW; Curtin L; Paulson L; Clark-Swanson K; Hawkins-Daarud A; Mitchell JR; Jackson PR; Swanson KR,"Identification of key phenotypic regions such as necrosis, contrast enhancement, and edema on magnetic resonance imaging (MRI) is important for understanding disease evolution and treatment response in patients with glioma. Manual delineation is time intensive and not feasible for a clinical workflow. Automating phenotypic region segmentation overcomes many issues with manual segmentation, however, current glioma segmentation datasets focus on pre-treatment, diagnostic scans, where treatment effects and surgical cavities are not present. Thus, existing automatic segmentation models are not applicable to post-treatment imaging that is used for longitudinal evaluation of care. Here, we present a comparison of three-dimensional convolutional neural networks (nnU-Net architecture) trained on large temporally defined pre-treatment, post-treatment, and mixed cohorts. We used a total of 1563 imaging timepoints from 854 patients curated from 13 different institutions as well as diverse public data sets to understand the capabilities and limitations of automatic segmentation on glioma images with different phenotypic and treatment appearance. We assessed the performance of models using Dice coefficients on test cases from each group comparing predictions with manual segmentations generated by trained technicians. We demonstrate that training a combined model can be as effective as models trained on just one temporal group. The results highlight the importance of a diverse training set, that includes images from the course of disease and with effects from treatment, in the creation of a model that can accurately segment glioma MRIs at multiple treatment time points.",2023,Jun,medRxiv,,,,,10.1101/2023.05.31.23290537,37333148,#2105,Ranjbar 2023,"",""
Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images.,Lefevre E; Bouilhol E; Chauvière A; Souleyreau W; Derieppe MA; Trotier AJ; Miraux S; Bikfalvi A; Ribot EJ; Nikolski M,"Lungs are the most frequent site of metastases growth. The amount and size of pulmonary metastases acquired from MRI imaging data are the important criteria to assess the efficacy of new drugs in preclinical models. While efficient solutions both for MR imaging and the downstream automatic segmentation have been proposed for human patients, both MRI lung imaging and segmentation in preclinical animal models remains challenging due to the physiological motion (respiratory and cardiac movements), to the low amount of protons in this organ and to the particular challenge of precise segmentation of metastases. As a consequence post-mortem analysis is currently required to obtain information on metastatic volume. In this work, we have developed a complete methodological pipeline for automated analysis of lungs and metastases in mice, consisting of an MR sequence for image acquisition and a deep learning method for automatic segmentation of both lungs and metastases. On one hand, we optimized an MR sequence for mouse lung imaging with high contrast for high detection sensitivity. On the other hand we developed DeepMeta, a multiclass U-Net 3+ deep learning model to automatically segment the images. To assess if the proposed deep learning pipeline is able to provide an accurate segmentation of both lungs and pulmonary metastases, we have longitudinally imaged mice with fast- and slow-growing metastasis. Fifty-five balb/c mice were injected with two different derivatives of renal carcinoma cells. Mice were imaged with a SG-bSSFP (self-gated balanced steady state free precession) sequence at different time points after the injection of cancer cells. Both lung and metastases segmentations were manually performed by experts. DeepMeta was trained to perform lung and metastases segmentation based on the resulting ground truth annotations. Volumes of lungs and of pulmonary metastases as well as the number of metastases per mouse were measured on a separate test dataset of MR images. Thanks to the SG method, the 3D bSSFP images of lungs were artifact-free, enabling the downstream detection and serial follow-up of metastases. Moreover, both lungs and metastases segmentation was accurately performed by DeepMeta as soon as they reached the volume of  ∼ 0.02 mm3 . Thus we were able to distinguish two groups of mice in terms of number and volume of pulmonary metastases as well as in terms of the slow versus fast patterns of growth of metastases. We have shown that our methodology combining SG-bSSFP with deep learning, enables processing of the whole animal lungs and is thus a viable alternative to histology alone.",2022,,Front Bioinform,2,,999700,,10.3389/fbinf.2022.999700,36304332,#2108,Lefevre 2022,"",""
Online hard example mining vs. fixed oversampling strategy for segmentation of new multiple sclerosis lesions from longitudinal FLAIR MRI.,Schmidt-Mengin M; Soulier T; Hamzaoui M; Yazdan-Panah A; Bodini B; Ayache N; Stankoff B; Colliot O,"Detecting new lesions is a key aspect of the radiological follow-up of patients with Multiple Sclerosis (MS), leading to eventual changes in their therapeutics. This paper presents our contribution to the MSSEG-2 MICCAI 2021 challenge. The challenge is focused on the segmentation of new MS lesions using two consecutive Fluid Attenuated Inversion Recovery (FLAIR) Magnetic Resonance Imaging (MRI). In other words, considering longitudinal data composed of two time points as input, the aim is to segment the lesional areas, which are present only in the follow-up scan and not in the baseline. The backbone of our segmentation method is a 3D UNet applied patch-wise to the images, and in which, to take into account both time points, we simply concatenate the baseline and follow-up images along the channel axis before passing them to the 3D UNet. Our key methodological contribution is the use of online hard example mining to address the challenge of class imbalance. Indeed, there are very few voxels belonging to new lesions which makes training deep-learning models difficult. Instead of using handcrafted priors like brain masks or multi-stage methods, we experiment with a novel modification to online hard example mining (OHEM), where we use an exponential moving average (i.e., its weights are updated with momentum) of the 3D UNet to mine hard examples. Using a moving average instead of the raw model should allow smoothing of its predictions and allow it to give more consistent feedback for OHEM.",2022,,Front Neurosci,16,,1004050,,10.3389/fnins.2022.1004050,36408404,#2115,Schmidt-Mengin 2022,"",""
Improving the detection of new lesions in multiple sclerosis with a cascaded 3D fully convolutional neural network approach.,Salem M; Ryan MA; Oliver A; Hussain KF; Lladó X,"Longitudinal magnetic resonance imaging (MRI) has an important role in multiple sclerosis (MS) diagnosis and follow-up. Specifically, the presence of new lesions on brain MRI scans is considered a robust predictive biomarker for the disease progression. New lesions are a high-impact prognostic factor to predict evolution to MS or risk of disability accumulation over time. However, the detection of this disease activity is performed visually by comparing the follow-up and baseline scans. Due to the presence of small lesions, misregistration, and high inter-/intra-observer variability, this detection of new lesions is prone to errors. In this direction, one of the last Medical Image Computing and Computer Assisted Intervention (MICCAI) challenges was dealing with this automatic new lesion quantification. The MSSEG-2: MS new lesions segmentation challenge offers an evaluation framework for this new lesion segmentation task with a large database (100 patients, each with two-time points) compiled from the OFSEP (Observatoire français de la sclérose en plaques) cohort, the French MS registry, including 3D T2-w fluid-attenuated inversion recovery (T2-FLAIR) images from different centers and scanners. Apart from a change in centers, MRI scanners, and acquisition protocols, there are more challenges that hinder the automated detection process of new lesions such as the need for large annotated datasets, which may be not easily available, or the fact that new lesions are small areas producing a class imbalance problem that could bias trained models toward the non-lesion class. In this article, we present a novel automated method for new lesion detection of MS patient images. Our approach is based on a cascade of two 3D patch-wise fully convolutional neural networks (FCNNs). The first FCNN is trained to be more sensitive revealing possible candidate new lesion voxels, while the second FCNN is trained to reduce the number of misclassified voxels coming from the first network. 3D T2-FLAIR images from the two-time points were pre-processed and linearly co-registered. Afterward, a fully CNN, where its inputs were only the baseline and follow-up images, was trained to detect new MS lesions. Our approach obtained a mean segmentation dice similarity coefficient of 0.42 with a detection F1-score of 0.5. Compared to the challenge participants, we obtained one of the highest precision scores (PPVL = 0.52), the best PPVL rate (0.53), and a lesion detection sensitivity (SensL of 0.53).",2022,,Front Neurosci,16,,1007619,,10.3389/fnins.2022.1007619,36507318,#2125,Salem 2022,"",""
Triplanar U-Net with lesion-wise voting for the segmentation of new lesions on longitudinal MRI studies.,Hitziger S; Ling WX; Fritz T; D'Albis T; Lemke A; Grilo J,"We present a deep learning method for the segmentation of new lesions in longitudinal FLAIR MRI sequences acquired at two different time points. In our approach, the 3D volumes are processed slice-wise across the coronal, axial, and sagittal planes and the predictions from the three orientations are merged using an optimized voting strategy. Our method achieved best F1 score (0.541) among all participating methods in the MICCAI 2021 challenge Multiple sclerosis new lesions segmentation (MSSEG-2). Moreover, we show that our method is on par with the challenge's expert neuroradiologists: on an unbiased ground truth, our method achieves results comparable to those of the four experts in terms of detection (F1 score) and segmentation accuracy (Dice score).",2022,,Front Neurosci,16,,964250,,10.3389/fnins.2022.964250,36033604,#2158,Hitziger 2022,"",""
Longitudinal detection of new MS lesions using deep learning.,Kamraoui RA; Mansencal B; Manjon JV; Coupé P,"The detection of new multiple sclerosis (MS) lesions is an important marker of the evolution of the disease. The applicability of learning-based methods could automate this task efficiently. However, the lack of annotated longitudinal data with new-appearing lesions is a limiting factor for the training of robust and generalizing models. In this study, we describe a deep-learning-based pipeline addressing the challenging task of detecting and segmenting new MS lesions. First, we propose to use transfer-learning from a model trained on a segmentation task using single time-points. Therefore, we exploit knowledge from an easier task and for which more annotated datasets are available. Second, we propose a data synthesis strategy to generate realistic longitudinal time-points with new lesions using single time-point scans. In this way, we pretrain our detection model on large synthetic annotated datasets. Finally, we use a data-augmentation technique designed to simulate data diversity in MRI. By doing that, we increase the size of the available small annotated longitudinal datasets. Our ablation study showed that each contribution lead to an enhancement of the segmentation accuracy. Using the proposed pipeline, we obtained the best score for the segmentation and the detection of new MS lesions in the MSSEG2 MICCAI challenge.",2022,,Front Neuroimaging,1,,948235,,10.3389/fnimg.2022.948235,37555158,#2159,Kamraoui 2022,"",""
Longitudinal Prediction of Infant MR Images With Multi-Contrast Perceptual Adversarial Learning.,Peng L; Lin L; Lin Y; Chen YW; Mo Z; Vlasova RM; Kim SH; Evans AC; Dager SR; Estes AM; McKinstry RC; Botteron KN; Gerig G; Schultz RT; Hazlett HC; Piven J; Burrows CA; Grzadzinski RL; Girault JB; Shen MD; Styner MA,"The infant brain undergoes a remarkable period of neural development that is crucial for the development of cognitive and behavioral capacities (Hasegawa et al., 2018). Longitudinal magnetic resonance imaging (MRI) is able to characterize the developmental trajectories and is critical in neuroimaging studies of early brain development. However, missing data at different time points is an unavoidable occurrence in longitudinal studies owing to participant attrition and scan failure. Compared to dropping incomplete data, data imputation is considered a better solution to address such missing data in order to preserve all available samples. In this paper, we adapt generative adversarial networks (GAN) to a new application: longitudinal image prediction of structural MRI in the first year of life. In contrast to existing medical image-to-image translation applications of GANs, where inputs and outputs share a very close anatomical structure, our task is more challenging as brain size, shape and tissue contrast vary significantly between the input data and the predicted data. Several improvements over existing GAN approaches are proposed to address these challenges in our task. To enhance the realism, crispness, and accuracy of the predicted images, we incorporate both a traditional voxel-wise reconstruction loss as well as a perceptual loss term into the adversarial learning scheme. As the differing contrast changes in T1w and T2w MR images in the first year of life, we incorporate multi-contrast images leading to our proposed 3D multi-contrast perceptual adversarial network (MPGAN). Extensive evaluations are performed to assess the qualityand fidelity of the predicted images, including qualitative and quantitative assessments of the image appearance, as well as quantitative assessment on two segmentation tasks. Our experimental results show that our MPGAN is an effective solution for longitudinal MR image data imputation in the infant brain. We further apply our predicted/imputed images to two practical tasks, a regression task and a classification task, in order to highlight the enhanced task-related performance following image imputation. The results show that the model performance in both tasks is improved by including the additional imputed data, demonstrating the usability of the predicted images generated from our approach.",2021,,Front Neurosci,15,,653213,,10.3389/fnins.2021.653213,34566556,#2177,Peng 2021,"",""
Evaluation of a nnU-Net type automated clinical volumetric tumor segmentation tool for diffuse low-grade glioma follow-up.,Verdier M; Deverdun J; de Champfleur NM; Duffau H; Lam P; Santos TD; Troalen T; Maréchal B; Huelnhagen T; Bars EL,"BACKGROUND AND PURPOSE: Diffuse low-grade gliomas (DLGG) are characterized by a slow and continuous growth and always evolve towards an aggressive grade. Accurate prediction of the malignant transformation is essential as it requires immediate therapeutic intervention. One of its most precise predictors is the velocity of diameter expansion (VDE). Currently, the VDE is estimated either by linear measurements or by manual delineation of the DLGG on T2 FLAIR acquisitions. However, because of the DLGG's infiltrative nature and its blurred contours, manual measures are challenging and variable, even for experts. Therefore we propose an automated segmentation algorithm using a 2D nnU-Net, to 1) gain time and 2) standardize VDE assessment. MATERIALS AND METHODS: The 2D nnU-Net was trained on 318 acquisitions (T2 FLAIR & 3DT1 longitudinal follow-up of 30 patients, including pre- & post-surgery acquisitions, different scanners, vendors, imaging parameters…). Automated vs. manual segmentation performance was evaluated on 167 acquisitions, and its clinical interest was validated by quantifying the amount of manual correction required after automated segmentation of 98 novel acquisitions. RESULTS: Automated segmentation showed a good performance with a mean Dice Similarity Coefficient (DSC) of 0.82±0.13 with manual segmentation and a substantial concordance between VDE calculations. Major manual corrections (i.e., DSC<0.7) were necessary only in 3/98 cases and 81% of the cases had a DSC>0.9. CONCLUSION: The proposed automated segmentation algorithm can successfully segment DLGG on highly variable MRI data. Although manual corrections are sometimes necessary, it provides a reliable, standardized and time-winning support for VDE extraction to asses DLGG growth.",2023,Jun,J Neuroradiol,,,,,10.1016/j.neurad.2023.05.008,37308338,#2198,Verdier 2023,"",""
Longitudinal Assessment of Posttreatment Diffuse Glioma Tissue Volumes with Three-dimensional Convolutional Neural Networks.,Rudie JD; Calabrese E; Saluja R; Weiss D; Colby JB; Cha S; Hess CP; Rauschecker AM; Sugrue LP; Villanueva-Meyer JE,"Neural networks were trained for segmentation and longitudinal assessment of posttreatment diffuse glioma. A retrospective cohort (from January 2018 to December 2019) of 298 patients with diffuse glioma (mean age, 52 years ± 14 [SD]; 177 men; 152 patients with glioblastoma, 72 patients with astrocytoma, and 74 patients with oligodendroglioma) who underwent two consecutive multimodal MRI examinations were randomly selected into training (n = 198) and testing (n = 100) samples. A posttreatment tumor segmentation three-dimensional nnU-Net convolutional neural network with multichannel inputs (T1, T2, and T1 postcontrast and fluid-attenuated inversion recovery [FLAIR]) was trained to segment three multiclass tissue types (peritumoral edematous, infiltrated, or treatment-changed tissue [ED]; active tumor or enhancing tissue [AT]; and necrotic core). Separate longitudinal change nnU-Nets were trained on registered and subtracted FLAIR and T1 postlongitudinal images to localize and better quantify and classify changes in ED and AT. Segmentation Dice scores, volume similarities, and 95th percentile Hausdorff distances ranged from 0.72 to 0.89, 0.90 to 0.96, and 2.5 to 3.6 mm, respectively. Accuracy rates of the posttreatment tumor segmentation and longitudinal change networks being able to classify longitudinal changes in ED and AT as increased, decreased, or unchanged were 76%-79% and 90%-91%, respectively. The accuracy levels of the longitudinal change networks were not significantly different from those of three neuroradiologists (accuracy, 90%-92%; κ, 0.58-0.63; P > .05). The results of this study support the potential clinical value of artificial intelligence-based automated longitudinal assessment of posttreatment diffuse glioma. Keywords: MR Imaging, Neuro-Oncology, Neural Networks, CNS, Brain/Brain Stem, Segmentation, Quantification, Convolutional Neural Network (CNN) Supplemental material is available for this article. © RSNA, 2022.",2022,Sep,Radiol Artif Intell,4,5,e210243,,10.1148/ryai.210243,36204543,#2212,Rudie 2022,"",""
Modeling 4D Pathological Changes by Leveraging Normative Models.,Wang B; Prastawa M; Irimia A; Saha A; Liu W; Goh SY; Vespa PM; Van Horn JD; Gerig G,"With the increasing use of efficient multimodal 3D imaging, clinicians are able to access longitudinal imaging to stage pathological diseases, to monitor the efficacy of therapeutic interventions, or to assess and quantify rehabilitation efforts. Analysis of such four-dimensional (4D) image data presenting pathologies, including disappearing and newly appearing lesions, represents a significant challenge due to the presence of complex spatio-temporal changes. Image analysis methods for such 4D image data have to include not only a concept for joint segmentation of 3D datasets to account for inherent correlations of subject-specific repeated scans but also a mechanism to account for large deformations and the destruction and formation of lesions (e.g., edema, bleeding) due to underlying physiological processes associated with damage, intervention, and recovery. In this paper, we propose a novel framework that provides a joint segmentation-registration framework to tackle the inherent problem of image registration in the presence of objects not present in all images of the time series. Our methodology models 4D changes in pathological anatomy across time and and also provides an explicit mapping of a healthy normative template to a subject's image data with pathologies. Since atlas-moderated segmentation methods cannot explain appearance and locality pathological structures that are not represented in the template atlas, the new framework provides different options for initialization via a supervised learning approach, iterative semisupervised active learning, and also transfer learning, which results in a fully automatic 4D segmentation method. We demonstrate the effectiveness of our novel approach with synthetic experiments and a 4D multimodal MRI dataset of severe traumatic brain injury (TBI), including validation via comparison to expert segmentations. However, the proposed methodology is generic in regard to different clinical applications requiring quantitative analysis of 4D imaging representing spatio-temporal changes of pathologies.",2016,Oct,Comput Vis Image Underst,151,,3-13,,10.1016/j.cviu.2016.01.007,27818606,#2216,Wang 2016,"",""
A Longitudinal MRI Study of Amygdala and Hippocampal Subfields for Infants with Risk of Autism.,Li G; Chen MH; Wu D; Lian C; Sun Q; Shen D; Wang L,"Currently, there are still no early biomarkers to detect infants with risk of autism spectrum disorder (ASD), which is mainly diagnosed based on behavioral observations at three or four years of age. Since intervention efforts may miss a critical developmental window after 2 years old, it is clinically significant to identify imaging-based biomarkers at an early stage for better intervention, before behavioral diagnostic signs of ASD typically arising. Previous studies on older children and young adults with ASD demonstrate altered developmental trajectories of the amygdala and hippocampus. However, our knowledge on their developmental trajectories in early postnatal stages remains very limited. In this paper, for the first time, we propose a volume-based analysis of the amygdala and hippocampal subfields of the infant subjects with risk of ASD at 6, 12, and 24 months of age. To address the challenge of low tissue contrast and small structural size of infant amygdala and hippocampal subfields, we propose a novel deep-learning approach, dilated-dense U-Net, to digitally segment the amygdala and hippocampal subfields in a longitudinal dataset, the National Database for Autism Research (NDAR). A volume-based analysis is then performed based on the segmentation results. Our study shows that the overgrowth of amygdala and cornu ammonis sectors (CA) 1-3 May start from 6 months of age, which may be related to the emergence of autistic spectrum disorder.",2019,Oct,Graph Learn Med Imaging (2019),11849,,164-171,,10.1007/978-3-030-35817-4_20,32104792,#2237,Li 2019,"",""
CNN Detection of New and Enlarging Multiple Sclerosis Lesions from Longitudinal Mri Using Subtraction Images,N. M. Sepahvand; D. L. Arnold; T. Arbel,"Accurate detection and segmentation of new lesional activity in longitudinal Magnetic Resonance Images (MRIs) of patients with Multiple Sclerosis (MS) is important for monitoring disease activity, as well as for assessing treatment effects. In this work, we present the first deep learning framework to automatically detect and segment new and enlarging (NE) T2w lesions from longitudinal brain MRIs acquired from relapsing-remitting MS (RRMS) patients. The proposed framework is an adapted 3D U-Net [1] which includes as inputs the reference multi-modal MRI and T2-weighted lesion maps, as well an attention mechanism based on the subtraction MRI (between the two timepoints) which serves to assist the network in learning to differentiate between real anatomical change and artifactual change, while constraining the search space for small lesions. Experiments on a large, proprietary, multi -center, multi-modal, clinical trial dataset consisting of 1677 multi-modal scans illustrate that network achieves high overall detection accuracy (detection AUC=.95), outperforming (1) a U-Net without an attention mechanism (de-tection AUC=.93), (2) a framework based on subtracting independent T2-weighted segmentations (detection AUC=.57), and (3) DeepMedic (detection AUC=.84) [2], particularly for small lesions. In addition, the method was able to accurately classify patients as active/inactive with (sensitivities of. 69 and specificities of. 97).",2020,,2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI),,,127-130,,10.1109/ISBI45749.2020.9098554,,#2282,N 2020,"",""
Deep Learning for Volumetric Segmentation in Spatio-Temporal Data: Application to Segmentation of Prostate in DCE-MRI,J. Kang; G. Samarasinghe; U. Senanayake; S. Conjeti; A. Sowmya,"Segmentation of the prostate in MR images is an essential step that underpins the success of subsequent analysis methods, such as cancer lesion detection inside the tumour and registration between different modalities. This work focuses on leveraging deep learning for analysis of longitudinal volumetric datasets, particularly for the task of segmentation, and presents proof-of-concept for segmentation of the prostate in 3D+T DCE-MRI sequences. A two-stream processing pipeline is proposed for this task, comprising a spatial stream modelled using a volumetric fully convolutional network and a temporal stream modeled using recurrent neural networks with Long-Short-term Memory (LSTM) units. The predictions of the two streams are fused using deep neural networks. The proposed method has been validated on a public benchmark dataset of 17 patients, each with 40 temporal volumes. When averaged over three experiments, a highly competitive Dice overlap score of 0.8688 and sensitivity of 0.8694 were achieved. As a spatiotemporal segmentation method, it can easily migrate to other datasets.",2019,,2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019),,,61-65,,10.1109/ISBI.2019.8759314,,#2283,J 2019,"",""
TS-Net: A Deep Learning Framework for Automated Assessment of Longitudinal Tumor Volume Changes in an Orthotopic Breast Cancer Model Using MRI,Y. Jun; S. Jin; N. Myung; J. Jeong; J. Lee; H. J. Cho,"Monitoring tumor volume changes in response to therapeutic agents is a critical step in preclinical drug development. Here, an automated magnetic resonance imaging (MRI)-based approach is proposed using a deep learning framework for tracking longitudinal tumor volume changes in an orthotopic breast cancer model treated with chemotherapy. Longitudinal magnetic resonance images are employed to track changes in tumor volume over time, using an untreated group and a doxorubicin-treated group as the dataset to evaluate treatment effects. Our approach, called Tumor Segmentation-Net (TS-Net), involves replacing the encoder of U-Net with a pre-trained ResNet34 to improve performance. The model was trained using a sample size of n=19 from the untreated group and then subsequently assessed on both the untreated group (n=5) and treated group (n=6). The correlation between the tumor volume determined from the ground truth and that obtained from the trained output was strong ( $\text{R}^{2}$ =0.984, slope=0.996). These results can lead to automated three-dimensional visualization of different longitudinal volume changes with and without treatment. Notably, for small tumors with volumes between 2 and 5 mm3, the proposed TS-Net demonstrated an average Dice similarity coefficient score of 0.85, indicating the ability to reliably detect early tumors that may often be missed. Our approach offers a promising tool for preclinical evaluation of tumor volume changes and treatment efficacy in animal models.",2023,,IEEE Access,11,,55117-55125,,10.1109/ACCESS.2023.3281558,,#2284,Y 2023,"",""
Fully convolutional structured LSTM networks for joint 4D medical image segmentation,Y. Gao; J. M. Phillips; Y. Zheng; R. Min; P. T. Fletcher; G. Gerig,"Longitudinal medical image analysis has great potential to reveal developmental trajectories and monitor disease progression. This process relies on consistent and robust joint 4D segmentation. Traditional methods highly depend on the similarity of images over time and either build a template or assume the images could be co-registered. This process may fail when image sequences present major appearance changes. Recently, deep learning (DL) approaches have achieved state-of-the-art results for related challenges in computer vision. These approaches make use of models such as fully convolutional networks (FCNs) for end-to-end pixel-wise segmentation and recurrent neural networks (RNNs) with long short-term memory (LSTM) units for sequence-to-sequence modeling. In this paper, we propose a new DL framework called FCSLSTM for 4D image segmentation with FCNs for the spatial model and LSTM for the temporal model. This is the first DL framework with deep integration of FCNs and LSTM for joint 4D segmentation that could be trained end-to-end. Our approach achieves promising results with the demonstrated application to longitudinal pediatric magnetic resonance imaging (MRI) segmentation.",2018,,2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018),,,1104-1108,,10.1109/ISBI.2018.8363764,,#2289,Y 2018,"",""
Multi-view longitudinal CNN for multiple sclerosis lesion segmentation,"Birenbaum, A; Greenspan, H","In this work, a deep-learning based automated method for Multiple Sclerosis (MS) lesion segmentation is presented. Automatic segmentation of MS lesions is a challenging task due to their variability in shape, size, location and texture in Magnetic Resonance (MR) images. In the proposed scheme, MR intensities and White Matter (WM) priors are used to extract candidate lesion voxels, following which Convolutional Neural Networks (CNN) are utilized for false positive reduction and final segmentation result. The proposed network uses longitudinal data, a novel contribution in the domain of MS lesion analysis. The method obtained state-of-the-art results on the 2015 Longitudinal MS Lesion Segmentation Challenge dataset, and achieved a performance level equivalent to a trained human rater. Automatic segmentation methods, such as the one proposed, once proven in accuracy and robustness, can help diagnosis and patient follow-up while reducing the time consuming need of manual segmentation. (C) 2017 Elsevier Ltd. All rights reserved.",2017,,ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE,65,,111-118,WOS:000413388100010,10.1016/j.engappai.2017.06.006,,#2366,Birenbaum 2017,"",""
Consistent Segmentation of Longitudinal Brain MR Images with Spatio-Temporal Constrained Networks,"Wei, J; Shi, F; Cui, ZM; Pan, YS; Xia, Y; Shen, DG","Accurate and consistent segmentation of longitudinal brain magnetic resonance (MR) images is of great importance in studying brain morphological and functional changes over time. However, current available brain segmentation methods, especially deep learning methods, are mostly trained with cross-sectional brain images that might generate inconsistent results in longitudinal studies. To overcome this limitation, we present a novel coarse-to-fine spatio-temporal constrained deep learning model for consistent longitudinal segmentation based on limited labeled cross-sectional data with semi-supervised learning. Specifically, both segmentation smoothness and temporal consistency are imposed in the loss function. Moreover, brain structural changes over time are summarized as age constraint, to make the model better reflect the trends of longitudinal aging changes. We validate our proposed method on 53 sets of longitudinal T1-weighted brain MR images from ADNI, with an average of 4.5 time-points per subject. Both quantitative and qualitative comparisons with comparison methods demonstrate the superior performance of our proposed method.",2021,,"MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI 2021, PT I",12901,,89-98,WOS:000712019600009,10.1007/978-3-030-87193-2_9,,#2370,Wei 2021,"",""
Learning unbiased group-wise registration (LUGR) and joint segmentation: evaluation on longitudinal diffusion MRI,"Li, B; Niessen, WJ; Klein, S; Ikram, MA; Vernooij, MW; Bron, EE","Analysis of longitudinal changes in imaging studies often involves both segmentation of structures of interest and registration of multiple timeframes. The accuracy of such analysis could benefit from a tailored framework that jointly optimizes both tasks to fully exploit the information available in the longitudinal data. Most learning-based registration algorithms, including joint optimization approaches, currently suffer from bias due to selection of a fixed reference frame and only support pairwise transformations. We here propose an analytical framework based on an unbiased learning strategy for group-wise registration that simultaneously registers images to the mean space of a group to obtain consistent segmentations. We evaluate the proposed method on longitudinal analysis of a white matter tract in a brain MRI dataset with 2-3 time-points for 3249 individuals, i.e., 8045 images in total. The reproducibility of the method is evaluated on test-retest data from 97 individuals. The results confirm that the implicit reference image is an average of the input image. In addition, the proposed framework leads to consistent segmentations and significantly lower processing bias than that of a pair-wise fixed-reference approach. This processing bias is even smaller than those obtained when translating segmentations by only one voxel, which can be attributed to subtle numerical instabilities and interpolation. Therefore, we postulate that the proposed mean-space learning strategy could be widely applied to learning-based registration tasks. In addition, this group-wise framework introduces a novel way for learning-based longitudinal studies by direct construction of an unbiased within-subject template and allowing reliable and efficient analysis of spatio-temporal imaging biomarkers.",2021,,MEDICAL IMAGING 2021: IMAGE PROCESSING,11596,,,WOS:000672800200016,10.1117/12.2580928,,#2373,Li 2021,"",""
Automated segmentation of long and short axis DENSE cardiovascular magnetic resonance for myocardial strain analysis using spatio-temporal convolutional neural networks,"Barbaroux, H; Kunze, KP; Neji, R; Nazir, MS; Pennell, DJ; Nielles-Vallespin, S; Scott, AD; Young, AA","BackgroundCine Displacement Encoding with Stimulated Echoes (DENSE) facilitates the quantification of myocardial deformation, by encoding tissue displacements in the cardiovascular magnetic resonance (CMR) image phase, from which myocardial strain can be estimated with high accuracy and reproducibility. Current methods for analyzing DENSE images still heavily rely on user input, making this process time-consuming and subject to inter-observer variability. The present study sought to develop a spatio-temporal deep learning model for segmentation of the left-ventricular (LV) myocardium, as spatial networks often fail due to contrast-related properties of DENSE images.Methods2D + time nnU-Net-based models have been trained to segment the LV myocardium from DENSE magnitude data in short- and long-axis images. A dataset of 360 short-axis and 124 long-axis slices was used to train the networks, from a combination of healthy subjects and patients with various conditions (hypertrophic and dilated cardiomyopathy, myocardial infarction, myocarditis). Segmentation performance was evaluated using ground-truth manual labels, and a strain analysis using conventional methods was performed to assess strain agreement with manual segmentation. Additional validation was performed using an externally acquired dataset to compare the inter- and intra-scanner reproducibility with respect to conventional methods.ResultsSpatio-temporal models gave consistent segmentation performance throughout the cine sequence, while 2D architectures often failed to segment end-diastolic frames due to the limited blood-to-myocardium contrast. Our models achieved a DICE score of 0.83 +/- 0.05 and a Hausdorff distance of 4.0 +/- 1.1 mm for short-axis segmentation, and 0.82 +/- 0.03 and 7.9 +/- 3.9 mm respectively for long-axis segmentations. Strain measurements obtained from automatically estimated myocardial contours showed good to excellent agreement with manual pipelines, and remained within the limits of inter-user variability estimated in previous studies.ConclusionSpatio-temporal deep learning shows increased robustness for the segmentation of cine DENSE images. It provides excellent agreement with manual segmentation for strain extraction. Deep learning will facilitate the analysis of DENSE data, bringing it one step closer to clinical routine.",2023,,JOURNAL OF CARDIOVASCULAR MAGNETIC RESONANCE,25,1,,WOS:000959473800001,10.1186/s12968-023-00927-y,,#2380,Barbaroux 2023,"",""
A Hybrid Deep Learning Framework for Integrated Segmentation and Registration: Evaluation on Longitudinal White Matter Tract Changes,"Li, B; Niessen, WJ; Klein, S; de Groot, M; Ikram, MA; Vernooij, MW; Bron, EE","To accurately analyze changes of anatomical structures in longitudinal imaging studies, consistent segmentation across multiple time-points is required. Existing solutions often involve independent registration and segmentation components. Registration between time-points is used either as a prior for segmentation in a subsequent time point or to perform segmentation in a common space. In this work, we propose a novel hybrid convolutional neural network (CNN) that integrates segmentation and registration into a single procedure. We hypothesize that the joint optimization leads to increased performance on both tasks. The hybrid CNN is trained by minimizing an integrated loss function composed of four different terms, measuring segmentation accuracy, similarity between registered images, deformation field smoothness, and segmentation consistency. We applied this method to the segmentation of white matter tracts, describing functionally grouped axonal fibers, using N = 8045 longitudinal brain MRI data of 3249 individuals. The proposed method was compared with two multistage pipelines using two existing segmentation methods combined with a conventional deformable registration algorithm. In addition, we assessed the added value of the joint optimization for segmentation and registration separately. The hybrid CNN yielded significantly higher accuracy, consistency and reproducibility of segmentation than the multistage pipelines, and was orders of magnitude faster. Therefore, we expect it can serve as a novel tool to support clinical and epidemiological analyses on understanding microstructural brain changes over time.",2019,,"MEDICAL IMAGE COMPUTING AND COMPUTER ASSISTED INTERVENTION - MICCAI 2019, PT III",11766,,645-653,WOS:000548733600072,10.1007/978-3-030-32248-9_72,,#2414,Li 2019,"",""
Longitudinal Multiple Sclerosis Lesion Segmentation Using Multi-view Convolutional Neural Networks,"Birenbaum, A; Greenspan, H","Automatic segmentation of Multiple Sclerosis (MS) lesions is a challenging task due to their variability in shape, size, location and texture in Magnetic Resonance (MR) images. A reliable, automatic segmentation method can help diagnosis and patient follow-up while reducing the time consuming need of manual segmentation. In this paper, we present a fully automated method for MS lesion segmentation. The proposed method uses MR intensities and White Matter (WM) priors for extraction of candidate lesion voxels and uses Convolutional Neural Networks for false positive reduction. Our networks process longitudinal data, a novel contribution in the domain of MS lesion analysis. The method was tested on the ISBI 2015 dataset and obtained state-of-the-art Dice results with the performance level of a trained human rater.",2016,,DEEP LEARNING AND DATA LABELING FOR MEDICAL APPLICATIONS,10008,,58-67,WOS:000389936900007,10.1007/978-3-319-46976-8_7,,#2420,Birenbaum 2016,"",""
Cascaded Convolutional Neural Networks for Spine Chordoma Tumor Segmentation from MRI,"Reza, SMS; Roy, S; Park, DM; Pham, DL; Butman, JA","Chordoma is a rare type of tumor that usually appears in the bone near the spinal cord and skull base. Due to their location in the skull base and diverse appearance in size and shape, automatic segmentation of chordoma tumors from magnetic resonance images (MRI) is a challenging task. In addition, similar MR intensity distributions of different anatomical regions, specifically sinuses, make the segmentation task from MRI more challenging. In comparison, most of the state-of-the-art lesion segmentation methods are designed to segment pathologies inside the brain. In this work, we propose an automatic chordoma segmentation framework using two cascaded 3D convolutional neural networks (CNN) via an auto-context model. While the first network learns to detect all potential tumor voxels, the second network fine-tunes the classifier to distinguish true tumor voxels from the false positives detected by the first network. The proposed method is evaluated using multi-contrast MR images of 22 longitudinal scans from 8 patients. Preliminary results showed a linear correlation of 0.71 between the detected and manually outlined tumor volumes, compared to 0 : 40 for a random forest (RF) based method. Furthermore, the response of tumor growth over time, i.e. increasing, decreasing, or stable, is evaluated according to the response evaluation criteria in solid tumors with an outcome of 0 : 26 kappa coefficient, compared to 0.13 for the RF based method.",2019,,"MEDICAL IMAGING 2019: BIOMEDICAL APPLICATIONS IN MOLECULAR, STRUCTURAL, AND FUNCTIONAL IMAGING",10953,,,WOS:000483014900069,10.1117/12.2514000,,#2483,Reza 2019,"",""
Learning from pseudo-labels: deep networks improve consistency in longitudinal brain volume estimation,"Zhan, G; Wang, DA; Cabezas, M; Bai, L; Kyle, K; Ouyang, WL; Barnett, M; Wang, CY","IntroductionBrain atrophy is a critical biomarker of disease progression and treatment response in neurodegenerative diseases such as multiple sclerosis (MS). Confounding factors such as inconsistent imaging acquisitions hamper the accurate measurement of brain atrophy in the clinic. This study aims to develop and validate a robust deep learning model to overcome these challenges; and to evaluate its impact on the measurement of disease progression. MethodsVoxel-wise pseudo-atrophy labels were generated using SIENA, a widely adopted tool for the measurement of brain atrophy in MS. Deformation maps were produced for 195 pairs of longitudinal 3D T1 scans from patients with MS. A 3D U-Net, namely DeepBVC, was specifically developed overcome common variances in resolution, signal-to-noise ratio and contrast ratio between baseline and follow up scans. The performance of DeepBVC was compared against SIENA using McLaren test-retest dataset and 233 in-house MS subjects with MRI from multiple time points. Clinical evaluation included disability assessment with the Expanded Disability Status Scale (EDSS) and traditional imaging metrics such as lesion burden. ResultsFor 3 subjects in test-retest experiments, the median percent brain volume change (PBVC) for DeepBVC and SIENA was 0.105 vs. 0.198% (subject 1), 0.061 vs. 0.084% (subject 2), 0.104 vs. 0.408% (subject 3). For testing consistency across multiple time points in individual MS subjects, the mean (& PLUSMN; standard deviation) PBVC difference of DeepBVC and SIENA were 0.028% (& PLUSMN; 0.145%) and 0.031% (& PLUSMN;0.154%), respectively. The linear correlation with baseline T2 lesion volume were r = -0.288 (p < 0.05) and r = -0.249 (p < 0.05) for DeepBVC and SIENA, respectively. There was no significant correlation of disability progression with PBVC as estimated by either method (p = 0.86, p = 0.84). DiscussionDeepBVC is a deep learning powered brain volume change estimation method for assessing brain atrophy used T1-weighted images. Compared to SIENA, DeepBVC demonstrates superior performance in reproducibility and in the context of common clinical scan variances such as imaging contrast, voxel resolution, random bias field, and signal-to-noise ratio. Enhanced measurement robustness, automation, and processing speed of DeepBVC indicate its potential for utilisation in both research and clinical environments for monitoring disease progression and, potentially, evaluating treatment effectiveness.",2023,,FRONTIERS IN NEUROSCIENCE,17,,,WOS:001032233600001,10.3389/fnins.2023.1196087,,#2535,Zhan 2023,"",""
Personalised predictive modelling with brain-inspired spiking neural networks of longitudinal MRI neuroimaging data and the case study of dementia,"Doborjeh, M; Doborjeh, Z; Merkin, A; Bahrami, H; Sumich, A; Krishnamurthi, R; Medvedev, ON; Crook-Rumsey, M; Morgan, C; Kirk, I; Sachdev, PS; Brodaty, H; Kang, K; Wen, W; Feigin, V; Kasabov, N","Background: Longitudinal neuroimaging provides spatiotemporal brain data (STBD) measurement that can be utilised to understand dynamic changes in brain structure and/or function underpinning cognitive activities. Making sense of such highly interactive information is challenging, given that the features manifest intricate temporal, causal relations between the spatially distributed neural sources in the brain.Methods: The current paper argues for the advancement of deep learning algorithms in brain-inspired spiking neural networks (SNN), capable of modelling structural data across time (longitudinal measurement) and space (anatomical components). The paper proposes a methodology and a computational architecture based on SNN for building personalised predictive models from longitudinal brain data to accurately detect, understand, and predict the dynamics of an individual's functional brain state. The methodology includes finding clusters of similar data to each individual, data interpolation, deep learning in a 3-dimensional brain-template structured SNN model, classification and prediction of individual outcome, visualisation of structural brain changes related to the predicted outcomes, interpretation of results, and individual and group predictive marker discovery.Results: To demonstrate the functionality of the proposed methodology, the paper presents experimental results on a longitudinal magnetic resonance imaging (MRI) dataset derived from 175 older adults of the internationally recognised community-based cohort Sydney Memory and Ageing Study (MAS) spanning 6 years of follow-up.Significance: The models were able to accurately classify and predict 2 years ahead of cognitive decline, such as mild cognitive impairment (MCI) and dementia with 95% and 91% accuracy, respectively. The proposed methodology also offers a 3-dimensional visualisation of the MRI models reflecting the dynamic patterns of regional changes in white matter hyperintensity (WMH) and brain volume over 6 years.Conclusion: The method is efficient for personalised predictive modelling on a wide range of neuroimaging longitudinal data, including also demographic, genetic, and clinical data. As a case study, it resulted in finding predictive markers for MCI and dementia as dynamic brain patterns using MRI data. (C) 2021 Elsevier Ltd. All rights reserved.",2021,,NEURAL NETWORKS,144,,522-539,WOS:000707983400005,10.1016/j.neunet.2021.09.013,,#2588,Doborjeh 2021,"Thomas Shaw (2024-04-11 18:28:55)(Screen): Segmentation???
; ",""
Evaluating the use of synthetic T1-w images in new T2 lesion detection in multiple sclerosis,"Valencia, L; Clerigues, A; Valverde, S; Salem, M; Oliver, A; Rovira, A; Llado, X","The assessment of disease activity using serial brain MRI scans is one of the most valuable strategies for monitoring treatment response in patients with multiple sclerosis (MS) receiving disease-modifying treatments. Recently, several deep learning approaches have been proposed to improve this analysis, obtaining a good trade-off between sensitivity and specificity, especially when using T1-w and T2-FLAIR images as inputs. However, the need to acquire two different types of images is time-consuming, costly and not always available in clinical practice. In this paper, we investigate an approach to generate synthetic T1-w images from T2-FLAIR images and subsequently analyse the impact of using original and synthetic T1-w images on the performance of a state-of-the-art approach for longitudinal MS lesion detection. We evaluate our approach on a dataset containing 136 images from MS patients, and 73 images with lesion activity (the appearance of new T2 lesions in follow-up scans). To evaluate the synthesis of the images, we analyse the structural similarity index metric and the median absolute error and obtain consistent results. To study the impact of synthetic T1-w images, we evaluate the performance of the new lesion detection approach when using (1) both T2-FLAIR and T1-w original images, (2) only T2-FLAIR images, and (3) both T2-FLAIR and synthetic T1-w images. Sensitivities of 0.75, 0.63, and 0.81, respectively, were obtained at the same false-positive rate (0.14) for all experiments. In addition, we also present the results obtained when using the data from the international MSSEG-2 challenge, showing also an improvement when including synthetic T1-w images. In conclusion, we show that the use of synthetic images can support the lack of data or even be used instead of the original image to homogenize the contrast of the different acquisitions in new T2 lesions detection algorithms.",2022,,FRONTIERS IN NEUROSCIENCE,16,,,WOS:000868445700001,10.3389/fnins.2022.954662,,#2869,Valencia 2022,"",""
